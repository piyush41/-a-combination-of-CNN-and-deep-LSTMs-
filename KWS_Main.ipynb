{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "KWS_Main.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pp-7qEHKlfYF"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import f1_score\n",
        "import os\n",
        "import pickle\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Reshape, BatchNormalization, Dropout\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, ReLU\n",
        "import numpy as np\n",
        "from scipy.io import wavfile\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.ticker import MaxNLocator\n",
        "from sklearn.metrics import accuracy_score, recall_score, matthews_corrcoef\n",
        "from sklearn.metrics import precision_score, f1_score, confusion_matrix\n",
        "from sklearn.metrics._plot.confusion_matrix import ConfusionMatrixDisplay\n",
        "from python_speech_features import logfbank\n",
        "from KWS_utils import *"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model():\n",
        "    \"\"\"\n",
        "    Feature extractor for hotword detection\n",
        "    :return: Model\n",
        "    \"\"\"\n",
        "\n",
        "    model = Sequential()\n",
        "\n",
        "    # Normalization layer\n",
        "    model.add(Reshape(input_shape=INPUT_SHAPE, target_shape=TARGET_SHAPE))\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    for num_filters in filters:\n",
        "        # Convolutional layers\n",
        "        model.add(Conv2D(num_filters, kernel_size=KERNEL_SIZE, padding=\"same\"))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(ReLU())\n",
        "\n",
        "        # Pooling\n",
        "        model.add(MaxPooling2D(pool_size=POOL_SIZE))\n",
        "        model.add(Dropout(DROPOUT))\n",
        "\n",
        "    # Classification layers\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(DENSE_1, name=\"features512\"))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(ReLU())\n",
        "    model.add(Dropout(DROPOUT))\n",
        "    model.add(Dense(DENSE_2, name=\"features256\"))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(ReLU())\n",
        "    model.add(Dropout(DROPOUT))\n",
        "    model.add(Dense(NUM_CLASSES, activation=\"softmax\"))\n",
        "\n",
        "    model.summary()\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "e0xAIIr4mcKx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model_train():\n",
        "    \"\"\"\n",
        "    Trains model which is used as a feature extractor\n",
        "    :return:None\n",
        "    \"\"\"\n",
        "\n",
        "    # Download data\n",
        "    downloadData(data_path=\"/input/speech_commands/\")\n",
        "\n",
        "    # Get data dictionary\n",
        "    dataDict = getDataDict(data_path=\"/input/speech_commands/\")\n",
        "\n",
        "    # Obtain dataframe for each dataset\n",
        "    trainDF = getDataframe(dataDict[\"train\"])\n",
        "    valDF = getDataframe(dataDict[\"val\"])\n",
        "    devDF = getDataframe(dataDict[\"dev\"])\n",
        "    testDF = getDataframe(dataDict[\"test\"])\n",
        "\n",
        "    print(\"Dataset statistics\")\n",
        "    print(\"Train files: {}\".format(trainDF.shape[0]))\n",
        "    print(\"Validation files: {}\".format(valDF.shape[0]))\n",
        "    print(\"Dev test files: {}\".format(devDF.shape[0]))\n",
        "    print(\"Test files: {}\".format(testDF.shape[0]))\n",
        "\n",
        "    # Use TF Data API for efficient data input\n",
        "    train_data, train_steps = getDataset(df=trainDF, batch_size=BATCH_SIZE, cache_file=\"train_cache\", shuffle=True)\n",
        "\n",
        "    val_data, val_steps = getDataset(df=valDF, batch_size=BATCH_SIZE, cache_file=\"val_cache\", shuffle=False)\n",
        "\n",
        "    model = create_model()\n",
        "    model.summary()\n",
        "\n",
        "    # Stop training if the validation accuracy doesn't improve\n",
        "    earlyStopping = EarlyStopping(monitor=\"val_loss\", patience=PATIENCE, verbose=1)\n",
        "\n",
        "    # Reduce LR on validation loss plateau\n",
        "    reduceLR = ReduceLROnPlateau(monitor=\"val_loss\", patience=PATIENCE, verbose=1)\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(\n",
        "        loss=\"sparse_categorical_crossentropy\",\n",
        "        optimizer=Adam(learning_rate=LEARNING_RATE),\n",
        "        metrics=[\"sparse_categorical_accuracy\"],\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    history = model.fit(\n",
        "        train_data.repeat(),\n",
        "        steps_per_epoch=train_steps,\n",
        "        validation_data=val_data.repeat(),\n",
        "        validation_steps=val_steps,\n",
        "        epochs=EPOCHS,\n",
        "        callbacks=[earlyStopping, reduceLR],\n",
        "    )\n",
        "\n",
        "    # Save model\n",
        "    print(\"Saving model\")\n",
        "    model.save(\"../models/sheila_kws.h5\")\n",
        "\n",
        "    # Save history data\n",
        "    print(\"Saving training history\")\n",
        "    with open(\"../models/sheila_kws_history.pickle\", \"wb\") as file:\n",
        "        pickle.dump(history.history, file, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "    plot_history(history=history)\n"
      ],
      "metadata": {
        "id": "_z92SbYvlpdF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model_test():\n",
        "    \"\"\"\n",
        "    Tests the Sheila hotword detector\n",
        "    :return: None\n",
        "    \"\"\"\n",
        "\n",
        "    # Download data\n",
        "    downloadData(data_path=\"/input/speech_commands/\")\n",
        "\n",
        "    # Get dictionary with files and labels\n",
        "    dataDict = getDataDict(data_path=\"/input/speech_commands/\")\n",
        "\n",
        "    # Obtain dataframe by merging dev and test dataset\n",
        "    devDF = getDataframe(dataDict[\"dev\"], include_unknown=True)\n",
        "    testDF = getDataframe(dataDict[\"test\"], include_unknown=True)\n",
        "\n",
        "    evalDF = pd.concat([devDF, testDF], ignore_index=True)\n",
        "\n",
        "    print(\"Test files: {}\".format(evalDF.shape[0]))\n",
        "\n",
        "    # Obtain Sheila - Other separated data\n",
        "    evalDF[\"class\"] = evalDF.apply(lambda row: 1 if row[\"category\"] == \"sheila\" else -1, axis=1)\n",
        "    evalDF.drop(\"category\", axis=1)\n",
        "    test_true_labels = evalDF[\"class\"].tolist()\n",
        "\n",
        "    eval_data, _ = getDataset(df=evalDF, batch_size=BATCH_SIZE, cache_file=\"kws_val_cache\", shuffle=False)\n",
        "\n",
        "    # Load trained model\n",
        "    model = load_model(\"../models/_kws.h5\")\n",
        "\n",
        "    layer_name = \"features256\"\n",
        "    feature_extractor = Model(inputs=model.input, outputs=model.get_layer(layer_name).output)\n",
        "\n",
        "    # Extract the feature embeddings and evaluate using SVM\n",
        "    X_test = feature_extractor.predict(eval_data, use_multiprocessing=True)\n",
        "\n",
        "\n",
        "    OC_Statistics(X_test, test_true_labels, \"sheila_cm_without_noise\")\n"
      ],
      "metadata": {
        "id": "sUYpPJjluYxf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"main\":\n",
        "  model_train()\n",
        "  model_test()\n"
      ],
      "metadata": {
        "id": "b9qMnBituYsj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}